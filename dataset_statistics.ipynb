{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from src.preprocess_data import preprocess_dataset, pca_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class distribution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make windows of size 30sec like in original paper. Let them slide 10sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished p1_d1. Took 0 minutes, 22 seconds\n",
      "Finished p1_d2. Took 0 minutes, 33 seconds\n",
      "Finished p2_d1. Took 0 minutes, 36 seconds\n",
      "Finished p2_d2. Took 0 minutes, 36 seconds\n",
      "Finished p3_d1. Took 0 minutes, 46 seconds\n",
      "Finished p3_d2. Took 0 minutes, 20 seconds\n",
      "Finished p4_d1. Took 0 minutes, 26 seconds\n",
      "Finished p4_d2. Took 0 minutes, 26 seconds\n",
      "Finished p5_d1. Took 0 minutes, 17 seconds\n",
      "Finished p5_d2. Took 0 minutes, 15 seconds\n",
      "Finished p6_d1. Took 0 minutes, 19 seconds\n",
      "Finished p6_d2. Took 0 minutes, 18 seconds\n",
      "Finished p7_d1. Took 0 minutes, 16 seconds\n",
      "Finished p7_d2. Took 0 minutes, 16 seconds\n",
      "Finished p8_d1. Took 0 minutes, 21 seconds\n",
      "Finished p8_d2. Took 0 minutes, 19 seconds\n",
      "Finished p9_d1. Took 0 minutes, 29 seconds\n",
      "Finished p9_d2. Took 0 minutes, 14 seconds\n",
      "Finished p10_d1. Took 0 minutes, 18 seconds\n",
      "Finished p10_d2. Took 0 minutes, 31 seconds\n"
     ]
    }
   ],
   "source": [
    "dfs = preprocess_dataset(\"condensed\", 3000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['p1_d1', 'p1_d2', 'p2_d1', 'p2_d2', 'p3_d1', 'p3_d2', 'p4_d1', 'p4_d2', 'p5_d1', 'p5_d2', 'p6_d1', 'p6_d2', 'p7_d1', 'p7_d2', 'p8_d1', 'p8_d2', 'p9_d1', 'p9_d2', 'p10_d1', 'p10_d2'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       person Not Reading English Japanese horizontal Japanese vertical\n",
      "0    person 1        5641     926                1014               987\n",
      "1    person 2        5673    1022                 871              1061\n",
      "2    person 3        5525     816                1309               827\n",
      "3    person 4        6394     771                 759               730\n",
      "4    person 5        4558     827                 831              1200\n",
      "5    person 6        4732    1188                 882               875\n",
      "6    person 7        3372     957                1085               843\n",
      "7    person 8        6204     706                 732               843\n",
      "8    person 9        4699     711                 806              1044\n",
      "9   person 10        6020     666                 959               847\n",
      "10        Sum       52818    8590                9248              9257\n"
     ]
    }
   ],
   "source": [
    "def get_class_dist(df):\n",
    "    cldtr = pd.DataFrame(columns=[\"person\", \"Not Reading\", \"English\", \"Japanese horizontal\", \"Japanese vertical\"])\n",
    "\n",
    "    for person in range(1,11,1):\n",
    "        d = np.array([0,0,0,0])\n",
    "        for day in range(1,3,1):\n",
    "            key = \"p{}_d{}\".format(person,day)\n",
    "            d  += (df[key].value_counts(subset=\"label\", sort = False).transpose().to_numpy())\n",
    "        df_ = pd.DataFrame({\"person\" : \"person {}\".format(person),  \"Not Reading\" : [d[0]], \"English\" : [d[1]], \"Japanese horizontal\" : [d[2]], \"Japanese vertical\": [d[3]] })\n",
    "        cldtr = pd.concat([cldtr, df_], axis = 0)\n",
    "\n",
    "    total = cldtr.sum().transpose().to_numpy()[1:]\n",
    "    df_ = pd.DataFrame({\"person\" : \"Sum\",  \"Not Reading\" : [total[0]], \"English\" : [total[1]], \"Japanese horizontal\" : [total[2]], \"Japanese vertical\": [total[3]] })\n",
    "    cldtr = pd.concat([cldtr, df_], axis = 0, ignore_index=True)  \n",
    "    return cldtr\n",
    "\n",
    "print(get_class_dist(dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36544, 17)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def  balance_class_distribution(ds : np.ndarray, randomize : bool = False) -> np.ndarray:\n",
    "    \"\"\"Performs undersampling on the majority class, to get a balanced distirbution of the dataset\n",
    "        Warning, dataset will be sorted by labels afterwards, if this is undesired, set ```randomize=True``` \n",
    "    Args:\n",
    "        ds (np.ndarray): dataset\n",
    "        randomize (bool): If True, randomizes the order of the samples, else they will be sorted\n",
    "\n",
    "    \"\"\"\n",
    "    # Get amount of smallest class\n",
    "    classes, classes_rep = np.unique(ds[:,-1], return_counts=True)\n",
    "    min_class_rep = min(classes_rep)\n",
    "\n",
    "    # We allow classes to have 10% more samples than minority class\n",
    "    thresh = int(min_class_rep * 1.1)\n",
    "    \n",
    "    new_ds = np.empty([0,ds.shape[1]])\n",
    "    for c,a in zip(classes, classes_rep):\n",
    "        ds_h = ds[ds[:,-1] == c]\n",
    "        \n",
    "        if a > thresh:\n",
    "            ind = np.random.choice(a,thresh,replace=False)\n",
    "            ds_h = ds_h[ind]\n",
    "        \n",
    "        new_ds = np.concatenate([new_ds,ds_h])\n",
    "    \n",
    "    if randomize:\n",
    "        np.random.shuffle(new_ds)\n",
    "        \n",
    "    return new_ds    \n",
    "    \n",
    "balance_class_distribution(np.concatenate([d.to_numpy() for d in dfs.values()])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       person Not Reading English Japanese horizontal Japanese vertical\n",
      "0    person 1         825     793                 783               825\n",
      "1    person 2         957     930                 871               917\n",
      "2    person 3         873     816                 873               827\n",
      "3    person 4         757     757                 725               721\n",
      "4    person 5         847     809                 809               847\n",
      "5    person 6         884     884                 848               840\n",
      "6    person 7         781     781                 750               742\n",
      "7    person 8         776     706                 732               769\n",
      "8    person 9         781     711                 747               753\n",
      "9   person 10         732     666                 732               732\n",
      "10        Sum        8213    7853                7870              7973\n"
     ]
    }
   ],
   "source": [
    "dfr_cut = {key : pd.DataFrame(balance_class_distribution(dfs[key].to_numpy()), columns = dfs[key].columns) for key in dfs.keys()}\n",
    "print(get_class_dist(dfr_cut))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9df5590c36242588aa2ec08b48d5460772f3434d8fd6d76e09f7949a0a7cd2ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
