{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "## KNN\n",
    "Lets first create a K-NN model. During \"training\" it saves all dataframes in an internal storrage\n",
    "During testing, it calculates the distance between the input and all stored samples and selects the (majority vote) label of the K nearest neighbors.\n",
    "\n",
    "A good K can be found by using k-folds cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from src.preprocess_data import preprocess_dataset\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN():\n",
    "    MAX_MATRIX_ENTRIES = 2e8\n",
    "    def __init__(self, num_cols : int, k : int ) -> None:\n",
    "        \"\"\"Creates an empty KNN classifier\n",
    "\n",
    "        Args:\n",
    "            num_cols (int): The number of columns (features) the data is going to have, including the label.\n",
    "            k (int): The number of neighbours that should be considered.\n",
    "        \"\"\"\n",
    "        self.data = np.empty([0,num_cols])\n",
    "        self.shape = self.data.shape\n",
    "        self.k = k\n",
    "        self.__verbose =  False\n",
    "        return\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __call__(self, *args, **kwds):\n",
    "        return self.top_k(*args, **kwds)\n",
    "\n",
    "    def verbose(self, verbose = True) -> None:\n",
    "        \"\"\"Sets the model to the verbose mode, where it comments on what it is doing.\n",
    "\n",
    "        Args:\n",
    "            verbose (bool, optional): Defaults to True.\n",
    "        \"\"\"\n",
    "        self.__verbose = True\n",
    "        return None\n",
    "    \n",
    "    def store_data(self,df : pd.DataFrame ) -> None:\n",
    "        \"\"\"Stores a single dataframe into the internal storage\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The data to be saved.\n",
    "        \"\"\" \n",
    "        self.data = np.concatenate([self.data, df.to_numpy()])\n",
    "        self.shape = self.data.shape\n",
    "        \n",
    "        if self.__verbose:\n",
    "            print(\"New shape is {}.\".format(self.shape))\n",
    "        # Reset internal\n",
    "        self.__dat = None\n",
    "        self.__lab = None\n",
    "        return\n",
    "    \n",
    "    def set_k(self,k:int) -> None:\n",
    "        \"\"\"Set another k hyperparameter\n",
    "\n",
    "        Args:\n",
    "            k (int): The new k\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "    \n",
    "    def top_k(self, datapoint : np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Calculates the label using the cosine similarity between each datapoint and the stored data points\n",
    "\n",
    "        Args:\n",
    "            datapoint (np.narray): An array of shape [N,C], where N is the number of datasamples (rows) and C the number of features (columns), not including the label\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: An array of shape [N], containing the label prediction \n",
    "        \"\"\"\n",
    "        assert datapoint.shape[1] == self.shape[1]-1\n",
    "        \n",
    "        datapoint = datapoint.copy()\n",
    "        \n",
    "        # Pepare internal data representation for calculation\n",
    "        if self.__dat == None:\n",
    "            \n",
    "            # Remove label column\n",
    "            self.__dat = self.data[:,:-1]\n",
    "            self.__lab = self.data[:,-1].astype(int)\n",
    "            \n",
    "            # Normalize each row to length 1\n",
    "            self.__dat = self.__dat  / np.linalg.norm(self.__dat, axis = 1, keepdims=True)\n",
    "\n",
    "        datapoint = datapoint  / np.linalg.norm(datapoint, axis = 1, keepdims=True)\n",
    "        \n",
    "        # Split input array into smaller ones if N*n is to big\n",
    "        step_size = max(1,int(KNN.MAX_MATRIX_ENTRIES // len(self)))\n",
    "        labels_accumulator = []\n",
    "        \n",
    "        max_i = math.ceil(len(datapoint)/ step_size)\n",
    "        \n",
    "        if self.__verbose:\n",
    "            print(\"Starts calculating on self.data ({}) and input ({}).\".format(self.shape, datapoint.shape))\n",
    "            _st_t = datetime.datetime.now()\n",
    "    \n",
    "        for i in range(max_i):\n",
    "            \n",
    "            if self.__verbose:\n",
    "                _en_t = datetime.datetime.now()\n",
    "                _t = _en_t - _st_t\n",
    "                if i == 0:\n",
    "                     _t_ex = datetime.timedelta(seconds = 0)\n",
    "                else:\n",
    "                    _t_ex = _t *(max_i/i)\n",
    "\n",
    "            \n",
    "                print(\"Finsihed datapoints {}/{} ({}/{})    {}m{}s/{}m{}s\".format(\n",
    "                    i * step_size, len(datapoint), \n",
    "                    i,max_i,\n",
    "                    _t.seconds//60, _t.seconds%60,\n",
    "                    _t_ex.seconds//60, _t_ex.seconds%60,\n",
    "                    ),\n",
    "                      end = \"\\r\")\n",
    "                \n",
    "            # Calculate dot product of matrices\n",
    "            cos_sim = datapoint[i*step_size:(i+1)*step_size] @ self.__dat.T\n",
    "            \n",
    "            # Get indices of highest value\n",
    "            ind = np.argpartition(cos_sim, -self.k, axis = 1)[:,-self.k:]\n",
    "       \n",
    "            # Get corresponding labels from internal data\n",
    "            labels = np.array([self.__lab[ind[i]] for i in range(len(ind))])     \n",
    "        \n",
    "            # Do majority vote for each input data point\n",
    "            labels = np.array([np.argmax(np.bincount(labels[i])) for i in range(len(labels))])\n",
    "\n",
    "            labels_accumulator.append(labels)\n",
    "        \n",
    "        if self.__verbose:\n",
    "            _en_t = datetime.datetime.now()\n",
    "            _t = _en_t - _st_t\n",
    "            _t_ex = _t\n",
    "\n",
    "        \n",
    "            print(\"Finsihed datapoints {}/{} ({}/{})    {}m{}s/{}m{}s\".format(\n",
    "                len(datapoint), len(datapoint), \n",
    "                max_i,max_i,\n",
    "                _t.seconds//60, _t.seconds%60,\n",
    "                _t_ex.seconds//60, _t_ex.seconds%60,\n",
    "                ),)\n",
    "\n",
    "        return np.concatenate(labels_accumulator)\n",
    "        \n",
    "\n",
    "def calculate_accuraccy(labels1 : np.ndarray, labels2 : np.ndarray) -> float:\n",
    "    \"\"\"Calculates the accuraccy of a prediction in refference to the ground-truth\n",
    "\n",
    "    Args:\n",
    "        labels1 (np.ndarray): Ground-truth or prediction of shape [N]\n",
    "        labels2 (np.ndarray): Ground-truth or prediction of shape [N]\n",
    "\n",
    "    Returns:\n",
    "        float: The accuraccy of the prediction\n",
    "    \"\"\"\n",
    "    assert labels1.shape == labels2.shape\n",
    "    \n",
    "    diff = labels1 - labels2\n",
    "    wrong = np.count_nonzero(diff)\n",
    "    correct = len(labels1) - wrong\n",
    "    \n",
    "    return correct/len(labels1) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    dfs = preprocess_dataset(\"condensed\", 10, False,persons=[1,2], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    knn = KNN(len(dfs[list(dfs.keys())[0]].columns), 5)\n",
    "    knn.shape\n",
    "    for k in dfs.keys():\n",
    "        knn.store_data(dfs[k])\n",
    "        print(knn.shape)\n",
    "    print(\"Training completed\")  \n",
    "\n",
    "    sample_length = 10\n",
    "    dp = dfs[\"p1_d1\"].to_numpy()[:sample_length,:-1]\n",
    "\n",
    "    print(\"\\n\\nStart testing\")\n",
    "    y = knn(dp)    \n",
    "    print(\"Accuracy was {}\".format(calculate_accuraccy(y,dfs[\"p1_d1\"].to_numpy()[:sample_length,-1])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze performance\n",
    "Show how good we can predict one day by the previous day per person\n",
    "\n",
    "Problems: It takes too long because data is too high-dimensional $~(|4e5| \\cdot |4e5|) = ~ 10^{11}$ matrix values that have to be computed\n",
    "\n",
    "Ideas to improve performance:\n",
    "- Use tensors on GPU → Not practicable with current memory usage\n",
    "\n",
    "Ideas to reduce dimensionality: \n",
    "- 1. Make bigger time windows (Reduce rows) → From 10 (= 0.1s) to 100 (=1s)\n",
    "- 2. Use PCA on stored data and input data (Reduce columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished p1_d1. Took 0 minutes, 18 seconds\n",
      "Finished p1_d2. Took 0 minutes, 18 seconds\n",
      "Finished p2_d1. Took 0 minutes, 20 seconds\n",
      "Finished p2_d2. Took 0 minutes, 20 seconds\n",
      "Finished p3_d1. Took 0 minutes, 17 seconds\n",
      "Finished p3_d2. Took 0 minutes, 14 seconds\n",
      "Finished p4_d1. Took 0 minutes, 16 seconds\n",
      "Finished p4_d2. Took 0 minutes, 17 seconds\n",
      "Finished p5_d1. Took 0 minutes, 18 seconds\n",
      "Finished p5_d2. Took 0 minutes, 10 seconds\n",
      "Finished p6_d1. Took 0 minutes, 12 seconds\n",
      "Finished p6_d2. Took 0 minutes, 13 seconds\n",
      "Finished p7_d1. Took 0 minutes, 9 seconds\n",
      "Finished p7_d2. Took 0 minutes, 11 seconds\n",
      "Finished p8_d1. Took 0 minutes, 14 seconds\n",
      "Finished p8_d2. Took 0 minutes, 14 seconds\n",
      "Finished p9_d1. Took 0 minutes, 14 seconds\n",
      "Finished p9_d2. Took 0 minutes, 9 seconds\n",
      "Finished p10_d1. Took 0 minutes, 14 seconds\n",
      "Finished p10_d2. Took 0 minutes, 14 seconds\n"
     ]
    }
   ],
   "source": [
    "dfs = preprocess_dataset(\"condensed\", 100, False, persons=list(range(1,11,1)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_acc_for_k(_dfs, k):\n",
    "    ov_acc = []\n",
    "\n",
    "\n",
    "    for p in range(1,11,1):\n",
    "        df = _dfs[\"p{}_d2\".format(p)]\n",
    "        knn = KNN(len(df.columns), k)\n",
    "        #print(\"\")\n",
    "        #knn.verbose()\n",
    "        \n",
    "        knn.store_data(dfs[\"p{}_d1\".format(p)])\n",
    "        \n",
    "        pred = knn(df.to_numpy()[:,:-1])\n",
    "        acc = calculate_accuraccy(pred, df.to_numpy()[:,-1] )\n",
    "        \n",
    "        #print(\"Accuraccy for p = {} is {:.2f}\".format(p,acc))\n",
    "        ov_acc.append(acc)\n",
    "        \n",
    "    print(\"\\nOverall accuraccy in predicting the second day was {}({}) for K = {}\".format(np.mean(ov_acc), np.std(ov_acc), k))\n",
    "    \n",
    "\n",
    "for K in [1,2,3,5,7,10,15]:\n",
    "     overall_acc_for_k(dfs, K)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9df5590c36242588aa2ec08b48d5460772f3434d8fd6d76e09f7949a0a7cd2ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
