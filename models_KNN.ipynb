{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "## KNN\n",
    "Lets first create a K-NN model. During \"training\" it saves all dataframes in an internal storrage\n",
    "During testing, it calculates the distance between the input and all stored samples and selects the (majority vote) label of the K nearest neighbors.\n",
    "\n",
    "A good K can be found by using k-folds cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from src.preprocess_data import preprocess_dataset, pca_dataset\n",
    "import datetime\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from collections import deque\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.5\n",
      "(array([50., 50., 75.]), array([100.,  50.,  60.]))\n"
     ]
    }
   ],
   "source": [
    "class KNN():\n",
    "    MAX_MATRIX_ENTRIES = 2e8\n",
    "\n",
    "    def __init__(self, num_cols: int, k: int) -> None:\n",
    "        \"\"\"Creates an empty KNN classifier\n",
    "\n",
    "        Args:\n",
    "            num_cols (int): The number of columns (features) the data is going to have, including the label.\n",
    "            k (int): The number of neighbours that should be considered.\n",
    "        \"\"\"\n",
    "        self.data = np.empty([0, num_cols])\n",
    "        self.shape = self.data.shape\n",
    "        self.k = k\n",
    "        self.__verbose = False\n",
    "        self.use_dims = None\n",
    "        self.__pc = None\n",
    "        return\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __call__(self, *args, **kwds):\n",
    "        return self.top_k(*args, **kwds)\n",
    "\n",
    "    def verbose(self, verbose=True) -> None:\n",
    "        \"\"\"Sets the model to the verbose mode, where it comments on what it is doing.\n",
    "\n",
    "        Args:\n",
    "            verbose (bool, optional): Defaults to True.\n",
    "        \"\"\"\n",
    "        self.__verbose = True\n",
    "        return None\n",
    "\n",
    "    def set_reduce_dimensions(self, num_leading_dim: int) -> None:\n",
    "        \"\"\"Sets KNN to use PCA in order to reduce the dimensionality of the data.\n",
    "\n",
    "        Args:\n",
    "            num_leading_dim (int): The number of PCs that should be used for the trasnformation.\n",
    "        \"\"\"\n",
    "        self.use_dims = num_leading_dim\n",
    "        return\n",
    "\n",
    "    def get_pca(self):\n",
    "        if self.use_dims is None:\n",
    "            raise Exception(\"PCA can only be performed after setting a number of dimensions for reduction. Use ```KNN.set_reduce_dimensions```\")\n",
    "        if self.__pc is None:\n",
    "            self.__pca(None)\n",
    "        return self.__pc.components_\n",
    "    \n",
    "    def store_data(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"Stores a single dataframe into the internal storage\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): The data to be saved.\n",
    "        \"\"\"\n",
    "        self.data = np.concatenate([self.data, df.to_numpy()])\n",
    "        self.shape = self.data.shape\n",
    "\n",
    "        if self.__verbose:\n",
    "            print(\"New shape is {}.\".format(self.shape))\n",
    "        # Reset internal\n",
    "        self.__dat = None\n",
    "        self.__lab = None\n",
    "        self.__pc = None\n",
    "        return\n",
    "\n",
    "    def set_k(self, k: int) -> None:\n",
    "        \"\"\"Set another k hyperparameter\n",
    "\n",
    "        Args:\n",
    "            k (int): The new k\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "\n",
    "    def __pca(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Performs PCA on the internal saved data to calculate the transformed data\"\"\"\n",
    "        # Check if saved PCs are available\n",
    "        \n",
    "        if self.__pc is None:\n",
    "            # Calculate PCs\n",
    "            self.__pc = PCA(n_components = self.use_dims).fit(self.data[:,:-1])\n",
    "            \n",
    "            self.data = np.concatenate([self.__pc.transform(self.data[:,:-1]), self.data[:,-1, None] ], axis = 1)\n",
    "            self.shape = self.data.shape\n",
    "            \n",
    "        if data is None:\n",
    "            return   \n",
    "        \n",
    "        # Transform data\n",
    "        data = self.__pc.transform(data)\n",
    "    \n",
    "        return data\n",
    "\n",
    "    def top_k(self, datapoint: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Calculates the label using the cosine similarity between each datapoint and the stored data points\n",
    "\n",
    "        Args:\n",
    "            datapoint (np.narray): An array of shape [N,C], where N is the number of datasamples (rows) and C the number of features (columns), not including the label\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: An array of shape [N], containing the label prediction \n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        datapoint = datapoint.copy()\n",
    "\n",
    "        if self.use_dims is not None:\n",
    "            datapoint = self.__pca(datapoint)\n",
    "        \n",
    "        assert datapoint.shape[1] == self.shape[1]-1\n",
    "        \n",
    "        \n",
    "        # Pepare internal data representation for calculation\n",
    "        if self.__dat == None:\n",
    "\n",
    "            # Remove label column\n",
    "            self.__dat = self.data[:, :-1]\n",
    "            self.__lab = self.data[:, -1].astype(int)\n",
    "\n",
    "            # Normalize each row to length 1\n",
    "            self.__dat = self.__dat / \\\n",
    "                np.linalg.norm(self.__dat, axis=1, keepdims=True)\n",
    "\n",
    "        datapoint = datapoint / \\\n",
    "            np.linalg.norm(datapoint, axis=1, keepdims=True)\n",
    "\n",
    "        # Split input array into smaller ones if N*n is to big\n",
    "        step_size = max(1, int(KNN.MAX_MATRIX_ENTRIES // len(self)))\n",
    "        labels_accumulator = []\n",
    "\n",
    "        max_i = math.ceil(len(datapoint) / step_size)\n",
    "\n",
    "        if self.__verbose:\n",
    "            print(\"Starts calculating on self.data ({}) and input ({}).\".format(\n",
    "                self.shape, datapoint.shape))\n",
    "            _st_t = datetime.datetime.now()\n",
    "\n",
    "        for i in range(max_i):\n",
    "\n",
    "            if self.__verbose:\n",
    "                _en_t = datetime.datetime.now()\n",
    "                _t = _en_t - _st_t\n",
    "                if i == 0:\n",
    "                    _t_ex = datetime.timedelta(seconds=0)\n",
    "                else:\n",
    "                    _t_ex = _t * (max_i/i)\n",
    "\n",
    "                print(\"Finsihed datapoints {}/{} ({}/{})    {}m{}s/{}m{}s\".format(\n",
    "                    i * step_size, len(datapoint),\n",
    "                    i, max_i,\n",
    "                    _t.seconds//60, _t.seconds % 60,\n",
    "                    _t_ex.seconds//60, _t_ex.seconds % 60,\n",
    "                ),\n",
    "                    end=\"\\r\")\n",
    "\n",
    "            # Calculate dot product of matrices\n",
    "            cos_sim = datapoint[i*step_size:(i+1)*step_size] @ self.__dat.T\n",
    "\n",
    "            # Get indices of highest value\n",
    "            ind = np.argpartition(cos_sim, -self.k, axis=1)[:, -self.k:]\n",
    "\n",
    "            # Get corresponding labels from internal data\n",
    "            labels = np.array([self.__lab[ind[i]] for i in range(len(ind))])\n",
    "\n",
    "            # Do majority vote for each input data point\n",
    "            labels = np.array([np.argmax(np.bincount(labels[i]))\n",
    "                              for i in range(len(labels))])\n",
    "\n",
    "            labels_accumulator.append(labels)\n",
    "\n",
    "        if self.__verbose:\n",
    "            _en_t = datetime.datetime.now()\n",
    "            _t = _en_t - _st_t\n",
    "            _t_ex = _t\n",
    "\n",
    "            print(\"Finsihed datapoints {}/{} ({}/{})    {}m{}s/{}m{}s\".format(\n",
    "                len(datapoint), len(datapoint),\n",
    "                max_i, max_i,\n",
    "                _t.seconds//60, _t.seconds % 60,\n",
    "                _t_ex.seconds//60, _t_ex.seconds % 60,\n",
    "            ),)\n",
    "\n",
    "        return np.concatenate(labels_accumulator)\n",
    "\n",
    "\n",
    "def calculate_accuraccy(true: np.ndarray, pred: np.ndarray) ->  float:\n",
    "    \"\"\"Calculates the accuraccy of a prediction in refference to the ground-truth\n",
    "\n",
    "    Args:\n",
    "        true (np.ndarray): Ground-truth of shape [N]\n",
    "        pred (np.ndarray): Prediction of shape [N]\n",
    "\n",
    "    Returns:\n",
    "        float: The accuracy over all datasamples\n",
    "    \"\"\"\n",
    "    assert true.shape == pred.shape\n",
    "\n",
    "    p = true-pred\n",
    "    wrong = np.count_nonzero(p)\n",
    "    correct = len(true) - wrong\n",
    "\n",
    "    return correct/len(true)* 100\n",
    "\n",
    "def precission_recall(true: np.ndarray, pred: np.ndarray) ->  tuple:\n",
    "    \"\"\"Calculates the accuraccy of a prediction in refference to the ground-truth per class\n",
    "\n",
    "    Args:\n",
    "        true (np.ndarray): Ground-truth of shape [N]\n",
    "        pred (np.ndarray): Prediction of shape [N]\n",
    "\n",
    "    Returns:\n",
    "        Tuple: precission and recall\n",
    "    \"\"\"\n",
    "    \n",
    "    precission = precision_score(true,pred, average = None)\n",
    "    recall =        recall_score(true,pred, average = None)\n",
    "\n",
    "    return (precission * 100,recall * 100)\n",
    "\n",
    "\n",
    "print(calculate_accuraccy(\n",
    "    np.array([0,1,1,2,2,2,2,2]),\n",
    "    np.array([0,1,2,1,2,2,2,0])\n",
    "      ))\n",
    "print(precission_recall(\n",
    "    np.array([0,1,1,2,2,2,2,2]),\n",
    "    np.array([0,1,2,1,2,2,2,0])\n",
    "      ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished p1_d1. Took 0 minutes, 17 seconds\n",
      "Finished p1_d2. Took 0 minutes, 19 seconds\n",
      "Finished p2_d1. Took 0 minutes, 17 seconds\n",
      "Finished p2_d2. Took 0 minutes, 18 seconds\n"
     ]
    }
   ],
   "source": [
    "if 0:\n",
    "    dfs = preprocess_dataset(\"condensed\", 10, False, persons=[1, 2], )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(420758, 17)\n",
      "(857298, 17)\n",
      "(1273277, 17)\n",
      "(1720523, 17)\n",
      "Training completed\n",
      "\n",
      "\n",
      "Start testing\n",
      "Starts calculating on self.data ((1720523, 6)) and input ((1000, 5)).\n",
      "Finsihed datapoints 1000/1000 (9/9)    0m53s/0m53s\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Uni\\Neurocognitive Computing\\Neurocog\\models_KNN.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Uni/Neurocognitive%20Computing/Neurocog/models_KNN.ipynb#W4sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mStart testing\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Uni/Neurocognitive%20Computing/Neurocog/models_KNN.ipynb#W4sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m y \u001b[39m=\u001b[39m knn(dp[:,:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Uni/Neurocognitive%20Computing/Neurocog/models_KNN.ipynb#W4sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAccuracy \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m  Recall \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39m*\u001b[39mcalculate_accuraccy(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Uni/Neurocognitive%20Computing/Neurocog/models_KNN.ipynb#W4sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     y, dp[:, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])))\n",
      "\u001b[1;32md:\\Uni\\Neurocognitive Computing\\Neurocog\\models_KNN.ipynb Cell 5\u001b[0m in \u001b[0;36mcalculate_accuraccy\u001b[1;34m(labels1, labels2)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Uni/Neurocognitive%20Computing/Neurocog/models_KNN.ipynb#W4sZmlsZQ%3D%3D?line=186'>187</a>\u001b[0m \u001b[39m\"\"\"Calculates the accuraccy of a prediction in refference to the ground-truth\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Uni/Neurocognitive%20Computing/Neurocog/models_KNN.ipynb#W4sZmlsZQ%3D%3D?line=187'>188</a>\u001b[0m \n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Uni/Neurocognitive%20Computing/Neurocog/models_KNN.ipynb#W4sZmlsZQ%3D%3D?line=188'>189</a>\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Uni/Neurocognitive%20Computing/Neurocog/models_KNN.ipynb#W4sZmlsZQ%3D%3D?line=193'>194</a>\u001b[0m \u001b[39m    Tuple: precission and recall\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Uni/Neurocognitive%20Computing/Neurocog/models_KNN.ipynb#W4sZmlsZQ%3D%3D?line=194'>195</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Uni/Neurocognitive%20Computing/Neurocog/models_KNN.ipynb#W4sZmlsZQ%3D%3D?line=195'>196</a>\u001b[0m \u001b[39massert\u001b[39;00m labels1\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m labels2\u001b[39m.\u001b[39mshape\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/Uni/Neurocognitive%20Computing/Neurocog/models_KNN.ipynb#W4sZmlsZQ%3D%3D?line=197'>198</a>\u001b[0m precission \u001b[39m=\u001b[39m precision_score(labels1,labels2)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Uni/Neurocognitive%20Computing/Neurocog/models_KNN.ipynb#W4sZmlsZQ%3D%3D?line=198'>199</a>\u001b[0m recall \u001b[39m=\u001b[39m recall(labels1,labels2)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Uni/Neurocognitive%20Computing/Neurocog/models_KNN.ipynb#W4sZmlsZQ%3D%3D?line=200'>201</a>\u001b[0m \u001b[39mreturn\u001b[39;00m (precission,recall)\n",
      "File \u001b[1;32md:\\Tools\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1757\u001b[0m, in \u001b[0;36mprecision_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1628\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprecision_score\u001b[39m(\n\u001b[0;32m   1629\u001b[0m     y_true,\n\u001b[0;32m   1630\u001b[0m     y_pred,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1636\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1637\u001b[0m ):\n\u001b[0;32m   1638\u001b[0m     \u001b[39m\"\"\"Compute the precision.\u001b[39;00m\n\u001b[0;32m   1639\u001b[0m \n\u001b[0;32m   1640\u001b[0m \u001b[39m    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1755\u001b[0m \u001b[39m    array([0.5, 1. , 1. ])\u001b[39;00m\n\u001b[0;32m   1756\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m     p, _, _, _ \u001b[39m=\u001b[39m precision_recall_fscore_support(\n\u001b[0;32m   1758\u001b[0m         y_true,\n\u001b[0;32m   1759\u001b[0m         y_pred,\n\u001b[0;32m   1760\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m   1761\u001b[0m         pos_label\u001b[39m=\u001b[39;49mpos_label,\n\u001b[0;32m   1762\u001b[0m         average\u001b[39m=\u001b[39;49maverage,\n\u001b[0;32m   1763\u001b[0m         warn_for\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mprecision\u001b[39;49m\u001b[39m\"\u001b[39;49m,),\n\u001b[0;32m   1764\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1765\u001b[0m         zero_division\u001b[39m=\u001b[39;49mzero_division,\n\u001b[0;32m   1766\u001b[0m     )\n\u001b[0;32m   1767\u001b[0m     \u001b[39mreturn\u001b[39;00m p\n",
      "File \u001b[1;32md:\\Tools\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1544\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1542\u001b[0m \u001b[39mif\u001b[39;00m beta \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1543\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mbeta should be >=0 in the F-beta score\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1544\u001b[0m labels \u001b[39m=\u001b[39m _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n\u001b[0;32m   1546\u001b[0m \u001b[39m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[0;32m   1547\u001b[0m samplewise \u001b[39m=\u001b[39m average \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msamples\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32md:\\Tools\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1365\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1363\u001b[0m         \u001b[39mif\u001b[39;00m y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1364\u001b[0m             average_options\u001b[39m.\u001b[39mremove(\u001b[39m\"\u001b[39m\u001b[39msamples\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1365\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1366\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mTarget is \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m but average=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m'\u001b[39m\u001b[39m. Please \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1367\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mchoose another average setting, one of \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (y_type, average_options)\n\u001b[0;32m   1368\u001b[0m         )\n\u001b[0;32m   1369\u001b[0m \u001b[39melif\u001b[39;00m pos_label \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39mNone\u001b[39;00m, \u001b[39m1\u001b[39m):\n\u001b[0;32m   1370\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1371\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNote that pos_label (set to \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m) is ignored when \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1372\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39maverage != \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m'\u001b[39m\u001b[39m (got \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m). You may use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1375\u001b[0m         \u001b[39mUserWarning\u001b[39;00m,\n\u001b[0;32m   1376\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted']."
     ]
    }
   ],
   "source": [
    "if 0:\n",
    "    knn = KNN(len(dfs[list(dfs.keys())[0]].columns), 5)\n",
    "    knn.shape\n",
    "    for k in dfs.keys():\n",
    "        knn.store_data(dfs[k])\n",
    "        print(knn.shape)\n",
    "    print(\"Training completed\")\n",
    "    \n",
    "    \n",
    "    dp = dfs[\"p1_d1\"].to_numpy()[:1000,]\n",
    "\n",
    "    knn.set_reduce_dimensions(5)\n",
    "    knn.get_pca()\n",
    "    \n",
    "    knn.verbose()\n",
    "    \n",
    "    print(\"\\n\\nStart testing\")\n",
    "    y = knn(dp[:,:-1])\n",
    "    print(\"Accuracy {}  Recall {}\".format(*calculate_accuraccy(\n",
    "        y, dp[:, -1])))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze performance\n",
    "Show how good we can predict one day by the previous day per person\n",
    "\n",
    "Problems: It takes too long because data is too high-dimensional $~(|4e5| \\cdot |4e5|) = ~ 10^{11}$ matrix values that have to be computed\n",
    "\n",
    "Ideas to improve performance:\n",
    "- Use tensors on GPU → Not practicable with current memory usage\n",
    "\n",
    "Ideas to reduce dimensionality: \n",
    "- 1. Make bigger time windows (Reduce rows) → From 10 (= 0.1s) to 3000 (=30s) \"Reason: Non reading can also have short glimpses of reading. Like in original study\"\n",
    "- 2. Use PCA on stored data and input data (Reduce columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished p1_d1. Took 0 minutes, 15 seconds\n",
      "Finished p1_d2. Took 0 minutes, 17 seconds\n",
      "Finished p2_d1. Took 0 minutes, 14 seconds\n",
      "Finished p2_d2. Took 0 minutes, 19 seconds\n",
      "Finished p3_d1. Took 0 minutes, 15 seconds\n",
      "Finished p3_d2. Took 0 minutes, 15 seconds\n",
      "Finished p4_d1. Took 0 minutes, 14 seconds\n",
      "Finished p4_d2. Took 0 minutes, 16 seconds\n",
      "Finished p5_d1. Took 0 minutes, 16 seconds\n",
      "Finished p5_d2. Took 0 minutes, 12 seconds\n",
      "Finished p6_d1. Took 0 minutes, 16 seconds\n",
      "Finished p6_d2. Took 0 minutes, 14 seconds\n",
      "Finished p7_d1. Took 0 minutes, 11 seconds\n",
      "Finished p7_d2. Took 0 minutes, 12 seconds\n",
      "Finished p8_d1. Took 0 minutes, 17 seconds\n",
      "Finished p8_d2. Took 0 minutes, 22 seconds\n",
      "Finished p9_d1. Took 0 minutes, 20 seconds\n",
      "Finished p9_d2. Took 0 minutes, 11 seconds\n",
      "Finished p10_d1. Took 0 minutes, 13 seconds\n",
      "Finished p10_d2. Took 0 minutes, 14 seconds\n"
     ]
    }
   ],
   "source": [
    "if 0:\n",
    "    dfs = preprocess_dataset(\n",
    "    \"condensed\", 3000, overlapping=False, persons=list(range(1, 11, 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_acc_for_k(_dfs, k):\n",
    "    ov_acc = []\n",
    "\n",
    "    for p in range(1,11, 1):\n",
    "        df = _dfs[\"p{}_d2\".format(p)]\n",
    "        knn = KNN(len(df.columns), k)\n",
    "        # print(\"\")\n",
    "        # knn.verbose()\n",
    "\n",
    "        knn.store_data(_dfs[\"p{}_d1\".format(p)])\n",
    "\n",
    "        pred = knn(df.to_numpy()[:, :-1])\n",
    "        acc = calculate_accuraccy(pred, df.to_numpy()[:, -1])\n",
    "\n",
    "        #print(\"Accuraccy for p = {} is {:.2f}\".format(p,acc))\n",
    "        ov_acc.append(acc)\n",
    "\n",
    "\n",
    "    print(\"\\nOverall accuraccy in predicting the second day was {:.2f}({:.2f}) for K = {}\".format(\n",
    "        np.mean(ov_acc), np.std(ov_acc), k))\n",
    "    \n",
    "    return ov_acc\n",
    "\n",
    "\n",
    "def log_and_plot_KNN(_dfs, name):\n",
    "    log = pd.DataFrame()\n",
    "    for K in [1, 2, 3, 5, 7, 10, 15]:\n",
    "        acc = overall_acc_for_k(_dfs, K)\n",
    "        \n",
    "        log = pd.concat([log, pd.DataFrame({\n",
    "            \"K\" : [str(K) for _ in range(len(acc))], \"acc\" : acc\n",
    "        })])\n",
    "        \n",
    "    sns.boxplot(data=log, x=\"acc\", y=\"K\")\n",
    "\n",
    "    sns.despine(trim = True)\n",
    "\n",
    "    plt.grid(axis = \"x\", alpha = 0.6)\n",
    "    plt.savefig(\"res/{}.png\".format(name))\n",
    "    plt.savefig(\"res/{}.pdf\".format(name))\n",
    "    plt.close()\n",
    "    return log"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all k the accuraccy in predicting day 2 from day 1 per person is around 58% for $1<k\\leq15$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "preprocess_dataset() got an unexpected keyword argument 'overlapping'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Uni\\Neurocognitive Computing\\Neurocog\\models_KNN.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Uni/Neurocognitive%20Computing/Neurocog/models_KNN.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m1\u001b[39m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Uni/Neurocognitive%20Computing/Neurocog/models_KNN.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     log \u001b[39m=\u001b[39m log_and_plot_KNN(_dfs \u001b[39m=\u001b[39m preprocess_dataset(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Uni/Neurocognitive%20Computing/Neurocog/models_KNN.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mcondensed\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m3000\u001b[39;49m, overlapping\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, normalize\u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Uni/Neurocognitive%20Computing/Neurocog/models_KNN.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                 name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mKNN_norm\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Uni/Neurocognitive%20Computing/Neurocog/models_KNN.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     log \u001b[39m=\u001b[39m log_and_plot_KNN(_dfs \u001b[39m=\u001b[39m preprocess_dataset(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Uni/Neurocognitive%20Computing/Neurocog/models_KNN.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcondensed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m3000\u001b[39m, overlapping\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, normalize\u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m),\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Uni/Neurocognitive%20Computing/Neurocog/models_KNN.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mKNN_raw\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Uni/Neurocognitive%20Computing/Neurocog/models_KNN.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     log \u001b[39m=\u001b[39m log_and_plot_KNN(_dfs \u001b[39m=\u001b[39m pca_dataset(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Uni/Neurocognitive%20Computing/Neurocog/models_KNN.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         preprocess_dataset(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Uni/Neurocognitive%20Computing/Neurocog/models_KNN.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcondensed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m3000\u001b[39m, overlapping\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, normalize\u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m), num_pc\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Uni/Neurocognitive%20Computing/Neurocog/models_KNN.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m                     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mKNN_norm_pca2\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: preprocess_dataset() got an unexpected keyword argument 'overlapping'"
     ]
    }
   ],
   "source": [
    "if 1:\n",
    "    log = log_and_plot_KNN(_dfs = preprocess_dataset(\n",
    "    \"condensed\",normalize= True),\n",
    "                name=\"KNN_norm\")\n",
    "\n",
    "    log = log_and_plot_KNN(_dfs = preprocess_dataset(\n",
    "        \"condensed\", normalize= False),\n",
    "                    name=\"KNN_raw\")\n",
    "\n",
    "    log = log_and_plot_KNN(_dfs = pca_dataset(\n",
    "        preprocess_dataset(\n",
    "        \"condensed\", normalize= True), num_pc=2),\n",
    "                    name=\"KNN_norm_pca2\")\n",
    "    log = log_and_plot_KNN(_dfs = pca_dataset(\n",
    "        preprocess_dataset(\n",
    "        \"condensed\", normalize= True), num_pc=5),\n",
    "                    name=\"KNN_norm_pca5\")\n",
    "    log = log_and_plot_KNN(_dfs = pca_dataset(\n",
    "        preprocess_dataset(\n",
    "        \"condensed\",normalize= True), num_pc=10),\n",
    "                    name=\"KNN_norm_pca10\")\n",
    "\n",
    "    log = log_and_plot_KNN(_dfs = pca_dataset(\n",
    "        preprocess_dataset(\n",
    "        \"condensed\", normalize= False), num_pc=2),\n",
    "                    name=\"KNN_raw_pca2\")\n",
    "    log = log_and_plot_KNN(_dfs = pca_dataset(\n",
    "        preprocess_dataset(\n",
    "        \"condensed\", normalize= False), num_pc=5),\n",
    "                    name=\"KNN_raw_pca5\")\n",
    "    log = log_and_plot_KNN(_dfs = pca_dataset(\n",
    "        preprocess_dataset(\n",
    "        \"condensed\", normalize= False), num_pc=10),\n",
    "                    name=\"KNN_raw_pca10\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Normalized** | **PCA** | **Best Acc**   | K        |\n",
    "|----------------|---------|----------------|----------|\n",
    "|        +       | None    | 58             | 7/**10**/15  |\n",
    "|        +       | 2       | 62             | 7/10/**15**   |\n",
    "|        +       | 5       | 57             | 7/10/15   |\n",
    "|        +       | 10       | 57             | **2**/10/15   |\n",
    "|        -       | None       | 59            | 2/10/**15**   |\n",
    "|        -       | 2       | 60             | 2/5/7/**10**/15   |\n",
    "|        -       | 5       | 58             | **2**   |\n",
    "|        -       | 10       | 61             | **2**   |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN\n",
    "We now try simple linear neural networks instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = preprocess_dataset(\n",
    "    \"condensed\", 100, overlapping=False, persons=list(range(1, 11, 1)))\n",
    "for key in [\"p1_d1\", \"p1_d2\"]:\n",
    "     value = dfs.pop(key)\n",
    "     dfs_test = {key : value }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Batch 0\n",
      "Training. Batch 2789/2789. Running Acc: 65.94%     Loss window: 0.9843\n",
      "Training. Batch 171/171. Running Acc: 64.17%\n",
      "Batch 1\n",
      "Training. Batch 2789/2789. Running Acc: 65.95%     Loss window: 0.9527\n",
      "Training. Batch 171/171. Running Acc: 64.17%\n",
      "Batch 2\n",
      "Training. Batch 2789/2789. Running Acc: 65.95%     Loss window: 0.9607\n",
      "Training. Batch 171/171. Running Acc: 64.17%\n",
      "Batch 3\n",
      "Training. Batch 2789/2789. Running Acc: 65.95%     Loss window: 0.9569\n",
      "Training. Batch 171/171. Running Acc: 64.16%\n",
      "Batch 4\n",
      "Training. Batch 2789/2789. Running Acc: 65.95%     Loss window: 0.9126\n",
      "Training. Batch 171/171. Running Acc: 64.17%\n"
     ]
    }
   ],
   "source": [
    "class Linear_Model(torch.nn.Module):\n",
    "    def __init__(self, in_features = 16, out_features = 4, num_hidden = []) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        # Add input and output to layers\n",
    "        num_hidden = [in_features] + num_hidden + [out_features]\n",
    "        layers = []\n",
    "        \n",
    "        for i in range(len(num_hidden)-1):\n",
    "            layers.append(\n",
    "                torch.nn.Linear(in_features=num_hidden[i], out_features= num_hidden[i+1]))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            \n",
    "        # Remove last ReLu\n",
    "        del layers[-1]\n",
    "            \n",
    "    \n",
    "        self.model = torch.nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def train_epoch(model : torch.nn.Module, dataloader : torch.utils.data.DataLoader, optimizer : torch.optim.Optimizer, loss_function = torch.nn.CrossEntropyLoss(), verbose = False) -> float:\n",
    "    \"\"\"Trains the model on the dataset\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to be trained.\n",
    "        dataloader (torch.utils.data.DataLoader): The dataset.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer for the given model\n",
    "\n",
    "    Returns:\n",
    "        float: The accuraccy of the model\n",
    "    \"\"\"\n",
    "    \n",
    "    __device = next(model.parameters()).device\n",
    "    model.train()\n",
    "    correct, total, acc = 0,0, 0\n",
    "    loss_window = deque()\n",
    "    loss_window.extend([0]*5)\n",
    "\n",
    "    \n",
    "    for itr, (datas,labels) in enumerate(dataloader):\n",
    "        if verbose:\n",
    "            print(\"Training. Batch {}/{}. Running Acc: {:.2f}%     Loss window: {:.4f}\".format(itr,len(dataloader), acc, sum(list(loss_window))/len(loss_window)), end = \"\\r\")\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Predict\n",
    "        output = model(datas.to(__device))\n",
    "        \n",
    "        # Metrics\n",
    "        _, pred = output.cpu().max(1, keepdims=True)\n",
    "        correct += pred.eq(labels).sum().item()\n",
    "        total += len(datas)\n",
    "        acc = correct / total * 100\n",
    "        \n",
    "        # Update\n",
    "        loss = loss_function(output, labels.squeeze(-1))\n",
    "        \n",
    "        loss_window.append(loss)\n",
    "        loss_window.popleft()\n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if verbose:\n",
    "        print(\"Training. Batch {}/{}. Running Acc: {:.2f}%\".format(itr + 1,len(dataloader), acc))\n",
    "        \n",
    "    return acc\n",
    "\n",
    "def test_epoch(model : torch.nn.Module, dataloader : torch.utils.data.DataLoader, verbose = False) -> float : \n",
    "    \"\"\"Evaluates the model on the dataset \n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to be evaluated.\n",
    "        dataloader (torch.utils.data.DataLoader): The dataset.\n",
    "\n",
    "    Returns:\n",
    "        float: The accuraccy of the model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct, total, acc = 0,0, 0\n",
    "   \n",
    "\n",
    "    for itr, (datas,labels) in enumerate(dataloader):\n",
    "        if verbose:\n",
    "            print(\"Training. Batch {}/{}. Running Acc: {:.2f}%\".format(itr,len(dataloader), acc), end = \"\\r\")\n",
    "       \n",
    "        # Predict\n",
    "        output = model(datas)\n",
    "        \n",
    "        # Metrics\n",
    "        _, pred = output.cpu().max(1, keepdims=True)\n",
    "        correct += pred.eq(labels).sum().item()\n",
    "        total += len(datas)\n",
    "        acc = correct / total * 100\n",
    "        \n",
    "    if verbose:\n",
    "        print(\"Training. Batch {}/{}. Running Acc: {:.2f}%\".format(itr + 1,len(dataloader), acc))\n",
    "        \n",
    "    return acc\n",
    "\n",
    "    \n",
    "def create_dataloader(datasets : dict, batch_size : int, shuffle : bool = True, ) -> torch.utils.data.DataLoader:\n",
    "    \"\"\"Creates a single dataloader from all data provided in datasets\n",
    "\n",
    "    Args:\n",
    "        datasets (dict): A dictionary of datasets\n",
    "        batch_size (int): The batch size forwarded to the data_loader\n",
    "        shuffle (bool, optional): If the samples sould be reshuffled each epoch. Defaults to True.\n",
    "        \n",
    "    Returns:\n",
    "        torch.utils.data.DataLoader: The dataloader\n",
    "    \"\"\"\n",
    "    # Get datasets \n",
    "    datasets = list(datasets.values())\n",
    "    \n",
    "    # Transform to numpy\n",
    "    datasets = [ds.to_numpy() if ds is pd.DataFrame else ds for ds in datasets]\n",
    "    \n",
    "    # Concat into single numpy matrix\n",
    "    datasets = np.concatenate(datasets)\n",
    "    \n",
    "    # Split data from label\n",
    "    data = torch.from_numpy(datasets[:,:-1].astype(np.float32))\n",
    "    labels = torch.from_numpy(datasets[:,-1, None].astype(np.int64))\n",
    "    \n",
    "    # Construct TensorDataset\n",
    "    dataloader = DataLoader(TensorDataset(data, labels), batch_size = batch_size, shuffle = shuffle, num_workers=0)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "def evaluate_model(model : torch.nn.Module, num_epochs, dfs = None):\n",
    "    \"\"\"Evaluates a model, by training/testing in a \"leave-one-participant-out\" fashion\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module)\n",
    "    \"\"\"\n",
    "\n",
    "    if dfs is None:\n",
    "        dfs = dfs = preprocess_dataset( \"condensed\", 100, overlapping=False,)\n",
    "    \n",
    "    # Iterate over all candidates to leave out\n",
    "    for leave_out in range(1,11,1) :\n",
    "        keys = [\"p{}_d1\".format(leave_out),\"p{}_d2\".format(leave_out)]\n",
    "        \n",
    "        loader_train = create_dataloader(dfs_,BATCH_SIZE,)\n",
    "        loader_test = create_dataloader(dfs_test,BATCH_SIZE,)\n",
    "\n",
    "\n",
    "\n",
    "model = Linear_Model(num_hidden=[])\n",
    "\n",
    "\n",
    "loader_train = create_dataloader(dfs_,BATCH_SIZE,)\n",
    "loader_test = create_dataloader(dfs_test,BATCH_SIZE,)\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(),lr = 1e-2, weight_decay=1e-5)\n",
    "\n",
    "\n",
    "print(\"Start Training\")\n",
    "for batch in range(5):\n",
    "    print(\"Epoch {}\".format(batch))\n",
    "    train_epoch(model,loader_train,optim, verbose =True)\n",
    "    test_epoch(model,loader_test, verbose =True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things we want to investigate\n",
    "- Linear model on condensed dataset\n",
    "    - Training acc ~70%, Testing acc ~60% → more layers do not introduce more generalization, only overfitting\n",
    "- Linear model on condensed dataset (unnormalized)\n",
    "\n",
    "- In person prediction: Linear model vs KNN\n",
    "\n",
    "- Linear on concatenated data\n",
    "- LSTM on concatenated data\n",
    "- CNN on concatenated data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9df5590c36242588aa2ec08b48d5460772f3434d8fd6d76e09f7949a0a7cd2ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
